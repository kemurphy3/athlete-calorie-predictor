# Athlete Calorie Predictor - Environment Variables Template
# Copy this file to .env and fill in your actual values
# The .env file is already in .gitignore and will NOT be committed to git

# =============================================================================
# STRAVA API CONFIGURATION
# =============================================================================
# Get these from https://www.strava.com/settings/api
STRAVA_ACCESS_TOKEN=your_strava_access_token_here
STRAVA_CLIENT_ID=your_strava_client_id_here
STRAVA_CLIENT_SECRET=your_strava_client_secret_here
STRAVA_REFRESH_TOKEN=your_strava_refresh_token_here

# =============================================================================
# DATA PATHS (Optional - defaults shown)
# =============================================================================
# Path to UCI PAMAP2 dataset CSV files
PAMAP2_PATH=external/uci_pamap2

# Path to Kaggle Fitbit-style dataset CSV files  
KAGGLE_FITBIT_PATH=external/kaggle_fitbit

# Output directory for generated datasets
DATA_DIR=data

# Archive directory for scripts backups
ARCHIVE_DIR=archived_scripts

# =============================================================================
# OUTPUT BEHAVIOR
# =============================================================================
# Whether to save CSV output in addition to parquet
SAVE_CSV=true

# Target number of rows for synthetic expansion (0 = no expansion)
SYNTHETIC_TARGET_ROWS=0

# =============================================================================
# IDENTITY DEFAULTS
# =============================================================================
# Used when external sources lack demographic information
DEFAULT_SEX=f
DEFAULT_AGE=33.0
DEFAULT_WEIGHT_KG=57.6

# =============================================================================
# USAGE EXAMPLES
# =============================================================================
# 1) Strava-only (set STRAVA_ACCESS_TOKEN):
#    python pipelines/build_dataset.py
#
# 2) With external datasets:
#    python pipelines/build_dataset.py
#
# 3) With synthetic expansion to 500k rows:
#    export SYNTHETIC_TARGET_ROWS=500000
#    python pipelines/build_dataset.py
#
# 4) Load from .env file (recommended):
#    python -m dotenv -f .env -- python pipelines/build_dataset.py
